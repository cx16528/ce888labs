# Lab6



## Lab Exercises 

- [ ] run ``python mnist.py`` and note somewhere the test accuracy score
- [ ] Modify the code to add one more layer of 64 ``relu`` units and record the score
- [ ] Modify the code so that you are able to add as many layers of ``relu`` units as you want, controlled by a variable called ``n_hidden_layers``
- [ ] Add a Dropout layer with strength of 0.5
- [ ] (Optional) play around with different scores and optimise on the number of layers, trying to find the optimal hyperparameters
- I find the 128 "relu" and 3 hidden layers the result accuracy is bigger than 98%.
